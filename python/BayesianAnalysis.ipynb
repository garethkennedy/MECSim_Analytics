{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# Parameter fit via Bayesian analysis\n",
    "\n",
    "Use the sweep results conducted with MECSim to get the best fit parameters as well as statistically rigorous error bars for each parameter. \n",
    "\n",
    "Bayesian probability updater is used to convert the least squares values output from MECSim analysis tools and stored in the results file into the probability that a particular set of parameters is the correct model for matching the experimental data given that the correct set is within the investigated range.\n",
    "\n",
    "This method will work for any number of parameters and can use prior probability data to improve the quality of the parameter fits.\n",
    "\n",
    "This file is structured with the input parameters at the start for ease of use. For those interested the methodology and theory are shown below.\n",
    "\n",
    "\n",
    "The contents of this notebook are:\n",
    "- <p><a href=\"#ref_paras\">Parameters</a></p>\n",
    "- <p><a href=\"#ref_plot_paras\">Plotting parameters (optional)</a></p>\n",
    "- <p><a href=\"#ref_readResults\">Read least squares results </a></p>\n",
    "- <p><a href=\"#ref_bayes_theory\">Bayesian analysis theory</a></p>\n",
    "- <p><a href=\"#ref_priors\">Set priors</a></p>\n",
    "- <p><a href=\"#ref_update\">Bayesian update</a></p>\n",
    "- <p><a href=\"#ref_posteriors\">Output posterior probabilities</a></p>\n",
    "- <p><a href=\"#ref_error_bars\">Calculate error bars for all parameters</a></p>\n",
    "- <p><a href=\"#ref_output_error_bars\">Output error bars</a></p>\n",
    "- <p><a href=\"#ref_plot_posteriors\">Plot posterior probabilities and errors</a></p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref_paras\"></a>\n",
    "## Parameters\n",
    "\n",
    "General parameters and filesnames which will be ignored depending on the options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set an artifical minimum for the metric value(s)\n",
    "epsilon = 1.0e-6\n",
    "\n",
    "# set marker label in results header to mark when input variables are finished\n",
    "header_marker = 'S'\n",
    "\n",
    "# default name for results input file which constains least squares fit metric(s)\n",
    "results_file_name_default = 'results.txt'\n",
    "\n",
    "# set how priors done\n",
    "iLoadPriors = False\n",
    "priors_filename = \"priors_sample.txt\"\n",
    "\n",
    "# save posterior file\n",
    "iSavePosterior = True\n",
    "posterior_filename = \"posterior.txt\"\n",
    "\n",
    "# save parameter best fits\n",
    "iSaveParameters = True\n",
    "opt_para_filename = \"opt_parameters.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"ref_plot_paras\"></a>\n",
    "## Plotting parameters (optional)\n",
    "\n",
    "Plot a contour plot for the final probabilities against two parameters selected by their column number in the results file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot output to png, pdf files)\n",
    "iPlotOutput = True\n",
    "# interactive mode for plotting to screen (turned off if in script mode)\n",
    "iPlotInteractive = True\n",
    "# display optimal fit\n",
    "iPlotFit = True\n",
    "# display error bars\n",
    "iPlotErrBars = True\n",
    "# logscale for probabilities (coloured)\n",
    "iPlotLogscale = True\n",
    "# select which columns to plot as parameters (x,y)\n",
    "ix = 0\n",
    "iy = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load system functions for inputting arguments at command line\n",
    "import sys\n",
    "# load numpy\n",
    "import numpy as np\n",
    "# load interpolation function that supports irregular grids\n",
    "from scipy.interpolate import NearestNDInterpolator\n",
    "# load plotting functions if requested\n",
    "if(iPlotOutput):\n",
    "    from matplotlib  import cm\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.mlab as ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results file name\n",
    "\n",
    "Determine results file name. There are two options:\n",
    "\n",
    "1. this file is run from the command line, so the first argument is the name of this python file (py rather than ipynb).\n",
    "2. running in interactive mode - through a jupyter notebook say. In which case we use the default file name set above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thisCodeName = 'BayesianAnalysis.py'\n",
    "nLength = len(thisCodeName)\n",
    "tailString = sys.argv[0]\n",
    "tailString = tailString[-nLength:]\n",
    "if(tailString==thisCodeName):\n",
    "    # next should be the file name\n",
    "    results_file_name = sys.argv[1]\n",
    "    iPlotInteractive = False\n",
    "else:\n",
    "    results_file_name = results_file_name_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref_readResults\"></a>\n",
    "## Read results data\n",
    "\n",
    "Use header line to determine the number of input parameters compared to the number of result values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = [line.rstrip('\\n') for line in open(results_file_name)]\n",
    "\n",
    "# first line is header line (remove $ from marker as well)\n",
    "header_line = lines[0].replace('$', '')\n",
    "header_line = header_line.split(',')\n",
    "\n",
    "# find index for the results metric column header\n",
    "output_first_index = header_line.index(header_market)\n",
    "\n",
    "# total length of data file (less header)\n",
    "n_data = len(lines)-1\n",
    "\n",
    "# loop read over data\n",
    "input_data = []\n",
    "for i in range(n_data):\n",
    "    input_data.append(np.fromstring(lines[i+1].strip(), dtype=float, sep=','))\n",
    "input_data = np.array(input_data)\n",
    "n_cols = np.shape(input_data)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort by parameters first - in case of an issue with loading priors later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_data = input_data[input_data[:,(output_first_index-1)].argsort()]\n",
    "for i in np.arange(output_first_index-2, -1, -1): # excludes stop so final is 0\n",
    "    input_data = input_data[input_data[:,0].argsort(kind='mergesort')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect into parameters and least squares metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_parameters = input_data[0:n_data, 0:(output_first_index)]\n",
    "input_metrics    = input_data[0:n_data, output_first_index:n_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref_bayes_theory\"></a>\n",
    "## Bayesian analysis\n",
    "\n",
    "Conceptually each set of parameters $x_i$ is treated as a different model $M_i$ that can potentially fit the experimental data from a given experiment $D_k$. This experimental data can either be the least squares fit for a single harmonic (or dc) or the resulting metric when all harmonic fit data is combined via respective weights.\n",
    "\n",
    "We seek to find the probability of each model ($M_i$) given the input of new experimental data ($D_k$), which we write as $P(M_i|D_k)$.\n",
    "\n",
    "To update the indiviual probabilites for each model via [Bayes theory](https://en.wikipedia.org/wiki/Bayes%27_theorem). Since we have so many potential models (each set of parameters tested) we can write Bayes theorem as:\n",
    "\n",
    "$$\n",
    "P(M_i|D_k) = \\frac{ P(D_k | M_i) P(M_i) } { \\sum_j P(D_k | M_j) P(M_j) }\n",
    "$$\n",
    "where the probability of the data given the model is related to the least squares metric comparing the experimental current response data ($D_k$) to the simulated current ($S_{ki}$) for the parameters ($x_i$) via\n",
    "$$\n",
    "P(D_k | M_i) \\propto \\frac{1}{S_{ki}}.\n",
    "$$\n",
    "\n",
    "Since the least squares metric can return zero for a perfect fit then we need to add a small term to ensure the probability does not go to infinity. Also we can remove the proportionality constant since any choice we make for it cancels out when we apply the form for Bayes theorem above. Thus\n",
    "$$\n",
    "P(D_k | M_i) = \\frac{1}{S_{ki} + \\epsilon}.\n",
    "$$\n",
    "\n",
    "where $\\epsilon$ is much smaller than one, set as **epsilon** $10^{-6}$ above.\n",
    "\n",
    "In applying this theorem the **prior** ($P(M_i)$) is the pre-existing belief that a given model ($M_i$) is correct. Adding new comparison data will update our beliefs and output the **posterior** probabilities ($P(M_i|D_k)$). If we have multiple data sets then the posterior values from our first set ($k=1$) can be used as the priors for the next set ($k=2$) and so on, further improving the accuracy of the posterior probabilities.\n",
    "\n",
    "Note that these probabilities will always sum to one $\\sum_i P(M_i) = 1$ which implicitly assumes that the true model is somewhere within the parameter range input. This could result in misleading interpretations if, for example, the true model was for parameters well outside this range or a different electrochemical mechanism all together.\n",
    "\n",
    "Back to <a href=\"#top\">top</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref_priors\"></a>\n",
    "## Priors\n",
    "\n",
    "Vector of probabilities with the same length as the number of parameter combinations (**n_data**)\n",
    "\n",
    "Loading priors is more tricky. Fine if all in the same grid where all **input_parameters** are the same.\n",
    "\n",
    "How do we update sets where we have no new data while correctly updating others?\n",
    "\n",
    "For now assume same grid! Still need to order first\n",
    "\n",
    "Back to <a href=\"#top\">top</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using uniform priors\n"
     ]
    }
   ],
   "source": [
    "if(iLoadPriors):\n",
    "    print('Loading priors from ', priors_filename)\n",
    "    priors_full = np.loadtxt(priors_filename, delimiter=\",\")\n",
    "    priors_full = priors_full[priors_full[:,(output_first_index-1)].argsort()]\n",
    "    for i in np.arange(output_first_index-2, -1, -1): # excludes stop so final is 0\n",
    "        priors_full = priors_full[priors_full[:,0].argsort(kind='mergesort')]\n",
    "    # extract vector in same order as input_parameters\n",
    "    # FOR NOW MUST HAVE SAME DATA LENGTH\n",
    "    priors = priors_full[0:n_data, output_first_index:n_cols]\n",
    "else:\n",
    "    print('Using uniform priors')\n",
    "    priors = np.full((n_data, 1), 1./n_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref_update\"></a>\n",
    "## Uses Bayes to update beliefs\n",
    "\n",
    "Loop over metrics and combine with Bayesian update.\n",
    "\n",
    "Back to <a href=\"#top\">top</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## some issue with coding for P_Mi_Dk != 1!!!! when not using single metric\n",
    "n_metrics = np.shape(input_metrics)[1]\n",
    "P_Mi = priors\n",
    "# get P(D_k | M_i)\n",
    "for k in range(n_metrics):\n",
    "    ls_ki = input_metrics[0:n_data, k]\n",
    "    P_Dk_Mi = 1.0/(ls_ki + epsilon)\n",
    "    P_Dk_Mi_P_Mi = P_Dk_Mi*np.transpose(P_Mi)\n",
    "    P_Mi_Dk = P_Dk_Mi_P_Mi / P_Dk_Mi_P_Mi.sum()\n",
    "    P_Mi = P_Mi_Dk\n",
    "# transform output into a array of correct length\n",
    "P_Mi_Dk = np.array(P_Mi_Dk[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.16666667],\n",
       "       [ 0.16666667],\n",
       "       [ 0.16666667],\n",
       "       [ 0.16666667],\n",
       "       [ 0.16666667],\n",
       "       [ 0.16666667]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this should be the length of n_data (=2 for this sample!)\n",
    "priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref_posteriors\"></a>\n",
    "## Save posteriors\n",
    "\n",
    "Calculate output posterior\n",
    "\n",
    "Back to <a href=\"#top\">top</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "total size of new array must be unchanged",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-165a5c781c70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_first_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mP_Mi_Dk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mposterior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: total size of new array must be unchanged"
     ]
    }
   ],
   "source": [
    "parameters = input_parameters.reshape(n_data, output_first_index)\n",
    "values = P_Mi_Dk.reshape(n_data, n_metrics)\n",
    "posterior = np.concatenate((parameters, values), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if(iSavePosterior):\n",
    "    np.savetxt(posterior_filename, posterior, fmt='%.8e', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref_error_bars\"></a>\n",
    "## Calculate error bars\n",
    "\n",
    "For each parameter calculate the standard deviation (each direction) based on the probabilities given all other parameters are fixed at their optimal values.\n",
    "\n",
    "### Build interpolator function\n",
    "\n",
    "Interpolate to get the $x_{\\pm \\sigma}$ for each direction - cut at $x_{min/max}$ to avoid extrapolation to unrealistic values. So if $0 < x < 1$ then $x_{+\\sigma}$ and $x_{-\\sigma}$ must also be in this range\n",
    "\n",
    "To determine the probabilities probabilities along each line we use a simple nearest neighbour interpolator in N dimensions. Note that if resolution is poor then the resulting interpolations along each line will be a series of discrete steps. Even in this low resolution extreme we still get quite good values for the error bars.\n",
    "\n",
    "Back to <a href=\"#top\">top</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interpolator = NearestNDInterpolator(x=parameters, y=values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop over each paramater\n",
    "\n",
    "Loop over each parameter along the range of possible values while fixing all other parameters to their optimal values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_res = 100\n",
    "opt_index = np.argmax(P_Mi_Dk)\n",
    "opt_paras = input_parameters[opt_index,:]\n",
    "opt_min = np.min(parameters, axis=0)\n",
    "opt_max = np.max(parameters, axis=0)\n",
    "opt_para_p = opt_paras.copy()\n",
    "opt_para_m = opt_paras.copy()\n",
    "for i in range(len(opt_paras)):\n",
    "    # create x_i line\n",
    "    opt_xi = np.linspace(opt_min[i], opt_max[i], num=n_res, endpoint=True)\n",
    "    val_xi = opt_xi.copy()\n",
    "    opt_test = opt_paras.copy()\n",
    "    for j in range(len(opt_xi)):\n",
    "        opt_test[i] = opt_xi[j]\n",
    "        val_xi[j] = interpolator(opt_test)\n",
    "    # scale in xi direction by sum_xi\n",
    "    sum_xi = np.sum(val_xi)\n",
    "    val_xi = val_xi/sum_xi\n",
    "    cum_xi = np.cumsum(val_xi)\n",
    "    # interpolate for med - 1 st.dev = 0.159\n",
    "    opt_para_m[i] = np.interp(0.159, cum_xi, opt_xi)\n",
    "    # interpolate for med + 1 st.dev = 0.841\n",
    "    opt_para_p[i] = np.interp(0.841, cum_xi, opt_xi)\n",
    "print(\"opt-1sd = \", opt_para_m)\n",
    "print(\"opt     = \", opt_paras)\n",
    "print(\"opt+1sd = \", opt_para_p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref_output_error_bars\"></a>\n",
    "### Output optimal and errors\n",
    "\n",
    "\n",
    "Each optimal value for parameter $x_i$ has a row in the output file with a format of:\n",
    "\n",
    "    parameter label (e.g. 'Ezero'), x_opt, x_opt - 1 standard deviation, x_opt + 1 standard deviation\n",
    "\n",
    "Back to <a href=\"#top\">top</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if(iSaveParameters):\n",
    "    # round values\n",
    "    opt_para_m_out = np.reshape(np.round(opt_para_m, 8), (len(opt_paras), 1))\n",
    "    opt_paras_out  = np.reshape(np.round(opt_paras,  8), (len(opt_paras), 1))\n",
    "    opt_para_p_out = np.reshape(np.round(opt_para_p, 8), (len(opt_paras), 1))\n",
    "    # reshape parameter headers\n",
    "    para_label = np.reshape(np.array(header_line[0:len(opt_paras_out)]), (len(opt_paras_out), 1))\n",
    "    # join to single np.array for output\n",
    "    opt_para_array_out = np.concatenate((para_label, opt_paras_out, opt_para_m_out, opt_para_p_out), axis=1)\n",
    "    # output to text file (since some text then set format to string for all - numbers come out as %f without problem)\n",
    "    np.savetxt(opt_para_filename, opt_para_array_out, fmt='%s', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref_plot_posteriors\"></a>\n",
    "## Plot posteriors (and error bars)\n",
    "\n",
    "### Plotter setup\n",
    "\n",
    "\n",
    "Back to <a href=\"#top\">top</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if(iPlotOutput):\n",
    "    # set names\n",
    "    x_name = header_line[ix]\n",
    "    y_name = header_line[iy]\n",
    "\n",
    "    # select data for plot with Bayesian probability\n",
    "    x = input_parameters[0:n_data, ix]\n",
    "    y = input_parameters[0:n_data, iy]\n",
    "    z = P_Mi_Dk\n",
    "\n",
    "    # find min/max for plotting variables\n",
    "    xplotmin = np.min(x)\n",
    "    xplotmax = np.max(x)\n",
    "    yplotmin = np.min(y)\n",
    "    yplotmax = np.max(y)\n",
    "\n",
    "    # optimal value and error bars\n",
    "    x_opt = opt_paras[ix]\n",
    "    y_opt = opt_paras[iy]\n",
    "    x_err_m = x_opt - opt_para_m[ix]\n",
    "    y_err_m = y_opt - opt_para_m[iy]\n",
    "    x_err_p = opt_para_p[ix] - x_opt\n",
    "    y_err_p = opt_para_p[iy] - y_opt    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting parameters\n",
    "\n",
    "Overall resolution, aesthetic buffer around plot region and enter interactive mode (if requested)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if(iPlotOutput):\n",
    "    # set overall data resolution\n",
    "    ny, nx = 100, 100\n",
    "\n",
    "    # buffer settings\n",
    "    bufFactor = 100.\n",
    "    bufX = (xplotmax - xplotmin)/bufFactor\n",
    "    bufY = (yplotmax - yplotmin)/bufFactor\n",
    "\n",
    "    # set to interactive mode\n",
    "    if(iPlotInteractive):\n",
    "        %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output plot\n",
    "\n",
    "Output to png and pdf - also to screen if interactive mode set above\n",
    "\n",
    "The default aesthetic settings and axes labels can be changed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if(iPlotOutput):\n",
    "    deltax = (xplotmax - xplotmin)/float(nx)\n",
    "    deltay = (yplotmax - yplotmin)/float(ny)\n",
    "    xmin = xplotmin-bufX\n",
    "    xmax = xplotmax+bufX\n",
    "    ymin = yplotmin-bufY\n",
    "    ymax = yplotmax+bufY\n",
    "    xi = np.arange(xmin-bufX, xmax+bufX, deltax)\n",
    "    yi = np.arange(ymin-bufY, ymax+bufY, deltay)\n",
    "    # convert to log(z) if requested\n",
    "    if(iPlotLogscale):\n",
    "        z = np.log10(z)\n",
    "    #zi = ml.griddata(x, y, zlog, xi, yi, interp='nn') # 'nn' for incomplete data; 'linear' if complete\n",
    "    zi = ml.griddata(x, y, z, xi, yi, interp='linear') # 'nn' for incomplete data; 'linear' if complete\n",
    "    #zi = ml.griddata(x, y, zlog, xi, yi, interp='linear') # 'nn' for incomplete data; 'linear' if complete\n",
    "\n",
    "    plt.figure(figsize=(8,6),dpi=100)\n",
    "    plt.rcParams['xtick.major.size'] = 5\n",
    "    plt.rcParams['xtick.major.width'] = 2\n",
    "    plt.rcParams['xtick.minor.size'] = 3\n",
    "    plt.rcParams['xtick.minor.width'] = 2\n",
    "    plt.rcParams['ytick.major.size'] = 5\n",
    "    plt.rcParams['ytick.major.width'] = 2\n",
    "    plt.rcParams['ytick.minor.size'] = 3\n",
    "    plt.rcParams['ytick.minor.width'] = 2\n",
    "    plt.rcParams['axes.linewidth'] = 2\n",
    "    plt.rcParams['lines.linewidth'] = 2\n",
    "    plt.rcParams['xtick.labelsize'] = 14\n",
    "    plt.rcParams['ytick.labelsize'] = 14\n",
    "    plt.rcParams['contour.negative_linestyle'] = 'solid'\n",
    "    plt.rcParams['xtick.direction'] = 'out'\n",
    "    plt.rcParams['ytick.direction'] = 'out'\n",
    "    plt.ticklabel_format(axis='y', style='sci', scilimits=(-2,2))\n",
    "    CS = plt.contour(xi, yi, zi, 10, linewidths = 0.5, colors = 'k')\n",
    "    # Define a class that forces representation of float to look a certain way\n",
    "    # This remove trailing zero so '1.0' becomes '1'\n",
    "    class nf(float):\n",
    "        def __repr__(self):\n",
    "            str = '%.2f' % (self.__float__(),)\n",
    "            if str[-1] == '0':\n",
    "                return '%.2f' % self.__float__()\n",
    "            else:\n",
    "                return '%.2f' % self.__float__()\n",
    "\n",
    "    # Recast levels to new class\n",
    "    CS.levels = [nf(val) for val in CS.levels]\n",
    "\n",
    "    # Label levels with specially formatted floats\n",
    "    if plt.rcParams[\"text.usetex\"]:\n",
    "        fmt = r'%r'\n",
    "    else:\n",
    "        fmt = '%r'\n",
    "    plt.pcolormesh(xi, yi, zi, cmap = plt.get_cmap('rainbow'))\n",
    "    if(iPlotLogscale):\n",
    "        plt.colorbar().set_label(label='log$_{10}$P',size=15)\n",
    "    else:\n",
    "        plt.colorbar().set_label(label='Probability',size=15)\n",
    "    # plot error bars\n",
    "    if(iPlotErrBars):\n",
    "        plt.errorbar(x=x_opt, y=y_opt, xerr=[[x_err_m], [x_err_p]], yerr=[[y_err_m], [y_err_p]], \n",
    "                     fmt='--o', ecolor='k', elinewidth=4, capsize=10, capthick=4)\n",
    "        plt.errorbar(x=x_opt, y=y_opt, xerr=[[x_err_m], [x_err_p]], yerr=[[y_err_m], [y_err_p]], \n",
    "                     fmt='--o', ecolor='w', elinewidth=2, capsize=8, capthick=2)\n",
    "    else: # label the contours\n",
    "        plt.clabel(CS, CS.levels, inline=True, fmt=fmt, fontsize=12)\n",
    "    # plot optimal values\n",
    "    if(iPlotFit):\n",
    "        plt.scatter(x_opt, y_opt, marker = 'o', c = 'white', s = 100, zorder = 10)\n",
    "\n",
    "    \n",
    "    # set x/y limits\n",
    "    plt.xlim(xmin, xmax)\n",
    "    plt.ylim(ymin, ymax)\n",
    "    # change x/y labels\n",
    "    plt.xlabel(x_name, fontsize=18)\n",
    "    plt.ylabel(y_name, fontsize=18)\n",
    "    # save figures to PDF and png\n",
    "    plt.savefig(\"bayesian_plot.pdf\", dpi=400)\n",
    "    plt.savefig(\"bayesian_plot.png\", dpi=400)\n",
    "    if(iPlotInteractive):\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
