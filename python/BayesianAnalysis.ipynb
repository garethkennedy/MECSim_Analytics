{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# Parameter fit via Bayesian analysis\n",
    "\n",
    "Use the parameter sample or sweep results conducted with MECSim to get the best fit parameters as well as statistically rigorous error bars for each parameter. \n",
    "\n",
    "Bayesian probability updater is used to convert the sum of squares values output from ``CompareSmoothed.py`` stored in the results file (``results.txt``) into the probability that a particular set of parameters is the correct model for matching the experimental data given that the correct set is within the investigated range.\n",
    "\n",
    "This method will work for any number of parameters and can use prior probability data to improve the quality of the parameter fits.\n",
    "\n",
    "This notebook can be run automatically as a python script (``BayesianAnalysis.py``) after running the parameter loop if setup in ``GenerateScript.ipynb`` or ``RandomlySampleRange.ipynb``. Doing so will use whatever general and plotting parameters set below and will not be interactive. Best proceedure is to edit and run this notebook using Jupyter to allow interactive analysis and error checking.\n",
    "\n",
    "This notebook is structured with the input parameters at the start for ease of use. For interested users the methodology and theory are shown below.\n",
    "\n",
    "The contents of this notebook are:\n",
    "- <p><a href=\"#ref_paras\">Parameters</a></p>\n",
    "- <p><a href=\"#ref_plot_paras\">Plotting parameters (optional)</a></p>\n",
    "- <p><a href=\"#ref_readResults\">Read sum of squares results </a></p>\n",
    "- <p><a href=\"#ref_bayes_theory\">Bayesian analysis theory</a></p>\n",
    "- <p><a href=\"#ref_priors\">Set priors</a></p>\n",
    "- <p><a href=\"#ref_update\">Bayesian update</a></p>\n",
    "- <p><a href=\"#ref_posteriors\">Output posterior probabilities</a></p>\n",
    "- <p><a href=\"#ref_error_bars\">Calculate error bars for all parameters</a></p>\n",
    "- <p><a href=\"#ref_output_error_bars\">Output error bars</a></p>\n",
    "- <p><a href=\"#ref_plot_posteriors\">Plot posterior probabilities and errors</a></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref_paras\"></a>\n",
    "## Parameters\n",
    "\n",
    "General parameters and file names which will be ignored depending on the specificed options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# default name for results input file which constains som of squares comparison metric(s)\n",
    "results_file_name_default = 'results.txt'\n",
    "\n",
    "# set marker label in results header to mark when input variables are finished\n",
    "header_marker = 'S'\n",
    "\n",
    "# set how priors done\n",
    "iLoadPriors = False\n",
    "priors_filename = \"priors_sample.txt\"\n",
    "\n",
    "# save posterior file\n",
    "iSavePosterior = True\n",
    "posterior_filename = \"posterior.txt\"\n",
    "\n",
    "# save parameter best fits\n",
    "iSaveParameters = True\n",
    "opt_para_filename = \"opt_parameters.txt\"\n",
    "\n",
    "# flag to perform optimial parameters finding including error bars\n",
    "#   (only useful for independent/simple runs at present)\n",
    "iDoParameterOptimization = True\n",
    "\n",
    "# Set an artifical minimum for the metric value(s) \n",
    "#   (to avoid Prob=1/0 if sum of squares = 0)\n",
    "epsilon = 1.0e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"ref_plot_paras\"></a>\n",
    "## Plotting parameters (optional)\n",
    "\n",
    "Plot a contour plot for the final probabilities against two parameters selected by their column number in the results file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot output to png, pdf files)\n",
    "iPlotOutput = True\n",
    "\n",
    "# interactive mode for plotting to screen (turned off if in script mode)\n",
    "iPlotInteractive = True\n",
    "\n",
    "# select which columns to plot as parameters (x,y)\n",
    "#   labels taken from column titles in results_file_name\n",
    "ix = 0\n",
    "iy = 2\n",
    "\n",
    "# display optimal fit\n",
    "iPlotFit = True\n",
    "\n",
    "# display error bars\n",
    "iPlotErrBars = False\n",
    "\n",
    "# logscale for probabilities (coloured)\n",
    "iPlotLogscale = False\n",
    "\n",
    "# turn black outlined contours on/off\n",
    "iUseContours = True\n",
    "\n",
    "# label contours\n",
    "iLabelContours = True\n",
    "\n",
    "# plot colour mesh (at least this or contours should be true)\n",
    "iPlotColourMesh = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load system functions for inputting arguments at command line\n",
    "import sys\n",
    "# load numpy\n",
    "import numpy as np\n",
    "# load interpolation function that supports irregular grids\n",
    "from scipy.interpolate import NearestNDInterpolator\n",
    "# load plotting functions if requested\n",
    "if(iPlotOutput):\n",
    "    from matplotlib import cm\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.mlab as ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results file name\n",
    "\n",
    "Determine the results file name to run the Bayesian analysis over. There are two options:\n",
    "\n",
    "1. this file is run from the command line, so the first argument is the name of this python file (py rather than ipynb) and the second argument (if given) is the results file.\n",
    "2. running in interactive mode e.g. via the jupyter notebook. In which case the default file name set above is used."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "thisCodeName = 'BayesianAnalysis.py'\n",
    "nLength = len(thisCodeName)\n",
    "tailString = sys.argv[0]\n",
    "tailString = tailString[-nLength:]\n",
    "if(tailString==thisCodeName):\n",
    "    # next should be the file name\n",
    "    results_file_name = sys.argv[1]\n",
    "    iPlotInteractive = False\n",
    "else:\n",
    "    results_file_name = results_file_name_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref_readResults\"></a>\n",
    "## Read results data\n",
    "\n",
    "Use header line to determine the number of input parameters compared to the number of result values"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "lines = [line.rstrip('\\n') for line in open(results_file_name)]\n",
    "\n",
    "# first line is header line (remove $ from marker as well)\n",
    "header_line = lines[0].replace('$', '')\n",
    "header_line = header_line.split(',')\n",
    "\n",
    "# find index for the results metric column header\n",
    "output_first_index = header_line.index(header_marker)\n",
    "\n",
    "# total length of data file (less header)\n",
    "n_data = len(lines)-1\n",
    "\n",
    "# loop read over data\n",
    "input_data = []\n",
    "for i in range(n_data):\n",
    "    input_data.append(np.fromstring(lines[i+1].strip(), dtype=float, sep=','))\n",
    "input_data = np.array(input_data)\n",
    "n_cols = np.shape(input_data)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort by parameters first - in case of an issue with loading priors later"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "input_data = input_data[input_data[:,(output_first_index-1)].argsort()]\n",
    "for i in np.arange(output_first_index-2, -1, -1): # excludes stop so final is 0\n",
    "    input_data = input_data[input_data[:,0].argsort(kind='mergesort')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect input parameters and sum of squares metrics"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "input_parameters = input_data[0:n_data, 0:(output_first_index)]\n",
    "input_metrics    = input_data[0:n_data, output_first_index:n_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref_bayes_theory\"></a>\n",
    "## Bayesian analysis theory\n",
    "\n",
    "Conceptually each set of parameters $x_i$ is treated as a different model $M_i$ that can potentially fit the experimental data from a given experiment $D_k$. This experimental data can either be the sum of squares difference for a single harmonic (or dc) or the resulting metric when all harmonic fit data is combined via respective weights.\n",
    "\n",
    "We seek to find the probability of each model ($M_i$) given the input of new experimental data ($D_k$), which we write as $P(M_i|D_k)$.\n",
    "\n",
    "The probabilites for each model are updated via [Bayes theory](https://en.wikipedia.org/wiki/Bayes%27_theorem) which is written as:\n",
    "\n",
    "$$\n",
    "P(M_i|D_k) = \\frac{ P(D_k | M_i) P(M_i) } { \\sum_j P(D_k | M_j) P(M_j) }\n",
    "$$\n",
    "\n",
    "where $P(M_i)$ is the prior belief in model $M_i$ and the probability of the data given the model is related to the sum of squares metric comparing the experimental current response data ($D_k$) to the simulated current ($S_{ki}$) for the parameters ($x_i$) via\n",
    "\n",
    "$$\n",
    "P(D_k | M_i) \\propto \\frac{1}{S_{ki}}.\n",
    "$$\n",
    "\n",
    "Since the sum of squares metric can return zero for a perfect fit then we need to add a small term to ensure the probability does not go to infinity. Also we can remove the proportionality constant since any choice we make for it cancels out when we apply the form for Bayes theorem above. Thus\n",
    "$$\n",
    "P(D_k | M_i) = \\frac{1}{S_{ki} + \\epsilon}.\n",
    "$$\n",
    "\n",
    "where $\\epsilon$ is much smaller than one, by default this is set as **epsilon** $10^{-6}$ above.\n",
    "\n",
    "In applying this theorem the **prior** $P(M_i)$ is the pre-existing belief that a given model ($M_i$) is correct. Adding new comparison data will update our beliefs and output the **posterior** probabilities $P(M_i|D_k)$. If we have multiple data sets then the posterior values from our first set ($k=1$) can be used as the priors for the next set ($k=2$) and so on, further improving the accuracy of the posterior probabilities.\n",
    "\n",
    "Note that these probabilities will always sum to one, i.e. $\\sum_i P(M_i) = 1$, which implicitly assumes that the true model is somewhere within the parameter range input. This could result in misleading interpretations if, for example, the true model was for parameters well outside this range or a different electrochemical mechanism all together.\n",
    "\n",
    "Back to <a href=\"#top\">top</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref_priors\"></a>\n",
    "## Setting Priors\n",
    "\n",
    "The priors are loaded in as a vector of probabilities with the same length as the number of parameter set combinations (**n_data**). Often their are no priors to be loaded, in which case they are assumed to be a uniform distribution.\n",
    "\n",
    "**Caution** loading the priors can be tricky if you are using random sampling since the previously constructed sets of parameters will be different from the new parameter sets values. This is not a problem if the same grid with the same spacing is used such that all **input_parameters** are the same.\n",
    "\n",
    "**Possible extention**: \n",
    "How do we update sets where we have no new data while correctly updating others?\n",
    "\n",
    "**Current status**: For we assume the same grid is used for the priors as for the new data. This is ordered first as the new data can be unordered due to how it was generated (eg. parallel runs or multiple runs concatenated into a single file)\n",
    "\n",
    "Back to <a href=\"#top\">top</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(iLoadPriors):\n",
    "    print('Loading priors from ', priors_filename)\n",
    "    priors_full = np.loadtxt(priors_filename, delimiter=\",\")\n",
    "    priors_full = priors_full[priors_full[:,(output_first_index-1)].argsort()]\n",
    "    for i in np.arange(output_first_index-2, -1, -1): # excludes stop so final is 0\n",
    "        priors_full = priors_full[priors_full[:,0].argsort(kind='mergesort')]\n",
    "    # extract vector in same order as input_parameters\n",
    "    # FOR NOW MUST HAVE SAME DATA LENGTH\n",
    "    priors = priors_full[0:n_data, output_first_index:n_cols]\n",
    "else:\n",
    "    print('Using uniform priors')\n",
    "    priors = np.full((n_data, 1), 1./n_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref_update\"></a>\n",
    "## Uses Bayes theorm to update beliefs for each model\n",
    "\n",
    "Loop over metrics and combine with Bayesian update.\n",
    "\n",
    "Back to <a href=\"#top\">top</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_metrics = np.shape(input_metrics)[1]\n",
    "P_Mi = priors\n",
    "posteriors_store = []\n",
    "# get P(D_k | M_i)\n",
    "for k in range(n_metrics):\n",
    "    sos_ki = input_metrics[0:n_data, k]\n",
    "    P_Dk_Mi = np.divide(1.0, sos_ki + epsilon)\n",
    "    P_Dk_Mi_P_Mi = np.array([P_Dk_Mi[i]*P_Mi[i] for i in range(n_data)])\n",
    "    P_sum = np.array(P_Dk_Mi_P_Mi).sum()\n",
    "    if(P_sum > 0.0):\n",
    "        P_Mi_Dk = np.array([P_Dk_Mi_P_Mi[i] / P_sum for i in range(n_data)]).reshape(n_data)\n",
    "    else: # no new data - posterior = prior\n",
    "        P_Mi_Dk = P_Mi\n",
    "    # store posterior for this harmonic/metric\n",
    "    posteriors_store.append(P_Mi_Dk)\n",
    "    # set prior for next round to current posterior\n",
    "    P_Mi = P_Mi_Dk\n",
    "# transform output into a array of correct length\n",
    "posteriors_store = np.array(posteriors_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref_posteriors\"></a>\n",
    "## Save posteriors\n",
    "\n",
    "Save csv text file with the input metrics as well as the posterior values for each harmonic.\n",
    "\n",
    "Back to <a href=\"#top\">top</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   "outputs": [],
   "source": [
    "parameters = input_parameters.reshape(n_data, output_first_index)\n",
    "values = posteriors_store.reshape((n_metrics, n_data)).T\n",
    "posterior = np.concatenate((parameters, values), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(iSavePosterior):\n",
    "    np.savetxt(posterior_filename, posterior, fmt='%.8e', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref_error_bars\"></a>\n",
    "## Calculate error bars\n",
    "\n",
    "For each parameter calculate the standard deviation (in each direction) based on the probabilities given all other parameters are fixed at their optimal values.\n",
    "\n",
    "### Build interpolator function\n",
    "\n",
    "Interpolate to get the $x_{\\pm \\sigma}$ for each direction - cut at $x_{min/max}$ to avoid extrapolation to unrealistic values. So if $0 < x < 1$ then $x_{+\\sigma}$ and $x_{-\\sigma}$ must also be in this range\n",
    "\n",
    "To determine the probabilities probabilities along each line we use a simple nearest neighbour interpolator in N dimensions. Note that if resolution is poor then the resulting interpolations along each line will be a series of discrete steps. Even in this low resolution extreme we still get quite good values for the error bars.\n",
    "\n",
    "Back to <a href=\"#top\">top</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interpolator = NearestNDInterpolator(x=parameters, y=values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop over each parameter\n",
    "\n",
    "Loop over each parameter along the range of possible values while fixing all other parameters to their optimal values.\n",
    "\n",
    "**In development** - need to extend this to non-grid runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optimal parameters (default is they exist)\n",
    "iHaveOpt = True\n",
    "# find optimal parameters\n",
    "if(iDoParameterOptimization):\n",
    "    n_res = 100\n",
    "    opt_index = np.argmax(P_Mi_Dk)\n",
    "    opt_paras = input_parameters[opt_index,:]\n",
    "    opt_min = np.min(parameters, axis=0)\n",
    "    opt_max = np.max(parameters, axis=0)\n",
    "    opt_para_p = opt_paras.copy()\n",
    "    opt_para_m = opt_paras.copy()\n",
    "    for i in range(len(opt_paras)):\n",
    "        # create x_i line\n",
    "        opt_xi = np.linspace(opt_min[i], opt_max[i], num=n_res, endpoint=True)\n",
    "        val_xi = opt_xi.copy()\n",
    "        opt_test = opt_paras.copy()\n",
    "        for j in range(len(opt_xi)):\n",
    "            opt_test[i] = opt_xi[j]\n",
    "            val_xi[j] = interpolator(opt_test)\n",
    "        # scale in xi direction by sum_xi\n",
    "        sum_xi = np.sum(val_xi)\n",
    "        val_xi = val_xi/sum_xi\n",
    "        cum_xi = np.cumsum(val_xi)\n",
    "        # interpolate for med - 1 st.dev = 0.159\n",
    "        opt_para_m[i] = np.interp(0.159, cum_xi, opt_xi)\n",
    "        # interpolate for med + 1 st.dev = 0.841\n",
    "        opt_para_p[i] = np.interp(0.841, cum_xi, opt_xi)\n",
    "    print(\"opt-1sd = \", opt_para_m)\n",
    "    print(\"opt     = \", opt_paras)\n",
    "    print(\"opt+1sd = \", opt_para_p)\n",
    "else:\n",
    "    # nothing to output so nothing to save\n",
    "    iSaveParameters = False\n",
    "    # Optimal parameters\n",
    "    iHaveOpt = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref_output_error_bars\"></a>\n",
    "### Output optimal and errors\n",
    "\n",
    "\n",
    "Output the optimal value and error bars for parameter $x_i$ for each row in the output file with a format of:\n",
    "\n",
    "    parameter label (e.g. 'Ezero'), x_opt, x_opt - 1 standard deviation, x_opt + 1 standard deviation\n",
    "\n",
    "Back to <a href=\"#top\">top</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(iSaveParameters):\n",
    "    # round values\n",
    "    opt_para_m_out = np.reshape(np.round(opt_para_m, 8), (len(opt_paras), 1))\n",
    "    opt_paras_out  = np.reshape(np.round(opt_paras,  8), (len(opt_paras), 1))\n",
    "    opt_para_p_out = np.reshape(np.round(opt_para_p, 8), (len(opt_paras), 1))\n",
    "    # reshape parameter headers\n",
    "    para_label = np.reshape(np.array(header_line[0:len(opt_paras_out)]), (len(opt_paras_out), 1))\n",
    "    # join to single np.array for output\n",
    "    opt_para_array_out = np.concatenate((para_label, opt_paras_out, opt_para_m_out, opt_para_p_out), axis=1)\n",
    "    # output to text file (since some text then set format to string for all - numbers come out as %f without problem)\n",
    "    np.savetxt(opt_para_filename, opt_para_array_out, fmt='%s', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref_plot_posteriors\"></a>\n",
    "## Plot posteriors\n",
    "\n",
    "Plot the posterior probabilities for two selected parameters to ``bayesian_plot.pdf`` and ``bayesian_plot.png``. Settings such as contours or colour mesh plot as well as aesthetics are specified at the top of this notebook.\n",
    "\n",
    "\n",
    "Back to <a href=\"#top\">top</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(iPlotOutput):\n",
    "    # set names\n",
    "    x_name = header_line[ix]\n",
    "    y_name = header_line[iy]\n",
    "    \n",
    "    print(\"Plotting posterior probabilities for:\", x_name, y_name)\n",
    "    \n",
    "    # set to interactive mode if requested\n",
    "    if(iPlotInteractive):\n",
    "        %matplotlib inline\n",
    "\n",
    "    # select data for plot with Bayesian probability\n",
    "    x = input_parameters[0:n_data, ix]\n",
    "    y = input_parameters[0:n_data, iy]\n",
    "    z = P_Mi_Dk\n",
    "\n",
    "    # find min/max for plotting variables\n",
    "    xplotmin = np.min(x)\n",
    "    xplotmax = np.max(x)\n",
    "    yplotmin = np.min(y)\n",
    "    yplotmax = np.max(y)\n",
    "\n",
    "    # optimal value and error bars\n",
    "    if(iHaveOpt):\n",
    "        x_opt = opt_paras[ix]\n",
    "        y_opt = opt_paras[iy]\n",
    "        x_err_m = x_opt - opt_para_m[ix]\n",
    "        y_err_m = y_opt - opt_para_m[iy]\n",
    "        x_err_p = opt_para_p[ix] - x_opt\n",
    "        y_err_p = opt_para_p[iy] - y_opt\n",
    "    \n",
    "    # set overall data resolution\n",
    "    ny, nx = 100, 100\n",
    "\n",
    "    # buffer settings\n",
    "    bufFactor = 100.\n",
    "    bufX = (xplotmax - xplotmin)/bufFactor\n",
    "    bufY = (yplotmax - yplotmin)/bufFactor\n",
    "    \n",
    "    # calculate x, y axes grids\n",
    "    deltax = (xplotmax - xplotmin)/float(nx)\n",
    "    deltay = (yplotmax - yplotmin)/float(ny)\n",
    "    xmin = xplotmin-bufX\n",
    "    xmax = xplotmax+bufX\n",
    "    ymin = yplotmin-bufY\n",
    "    ymax = yplotmax+bufY\n",
    "    xi = np.arange(xmin-bufX, xmax+bufX, deltax)\n",
    "    yi = np.arange(ymin-bufY, ymax+bufY, deltay)\n",
    "    # convert to log(z) if requested\n",
    "    if(iPlotLogscale):\n",
    "        z = np.log10(z)\n",
    "    zi = ml.griddata(x, y, z, xi, yi, interp='linear') # 'nn' for incomplete data; 'linear' if complete\n",
    "\n",
    "    # setup aesthetics for figure\n",
    "    plt.figure(figsize=(8,6),dpi=100)\n",
    "    plt.rcParams['xtick.major.size'] = 5\n",
    "    plt.rcParams['xtick.major.width'] = 2\n",
    "    plt.rcParams['xtick.minor.size'] = 3\n",
    "    plt.rcParams['xtick.minor.width'] = 2\n",
    "    plt.rcParams['ytick.major.size'] = 5\n",
    "    plt.rcParams['ytick.major.width'] = 2\n",
    "    plt.rcParams['ytick.minor.size'] = 3\n",
    "    plt.rcParams['ytick.minor.width'] = 2\n",
    "    plt.rcParams['axes.linewidth'] = 2\n",
    "    plt.rcParams['lines.linewidth'] = 2\n",
    "    plt.rcParams['xtick.labelsize'] = 14\n",
    "    plt.rcParams['ytick.labelsize'] = 14\n",
    "    plt.rcParams['contour.negative_linestyle'] = 'solid'\n",
    "    plt.rcParams['xtick.direction'] = 'out'\n",
    "    plt.rcParams['ytick.direction'] = 'out'\n",
    "    plt.ticklabel_format(axis='y', style='sci', scilimits=(-2,2))\n",
    "    # setup the contours (black outlines)\n",
    "    if(iUseContours):\n",
    "        CS = plt.contour(xi, yi, zi, 10, linewidths = 0.5, colors = 'k')\n",
    "        # Define a class that forces representation of float to look a certain way\n",
    "        # This remove trailing zero so '1.0' becomes '1'\n",
    "        class nf(float):\n",
    "            def __repr__(self):\n",
    "                str = '%.2f' % (self.__float__(),)\n",
    "                if str[-1] == '0':\n",
    "                    return '%.2f' % self.__float__()\n",
    "                else:\n",
    "                    return '%.2f' % self.__float__()\n",
    "        # Recast levels to new class\n",
    "        CS.levels = [nf(val) for val in CS.levels]\n",
    "        # label the contours\n",
    "        if(iLabelContours):\n",
    "            # Label levels with specially formatted floats\n",
    "            if plt.rcParams[\"text.usetex\"]:\n",
    "                fmt = r'%r'\n",
    "            else:\n",
    "                fmt = '%r'\n",
    "            plt.clabel(CS, CS.levels, inline=True, fmt=fmt, fontsize=12)\n",
    "\n",
    "        \n",
    "    if(iPlotColourMesh):\n",
    "        plt.pcolormesh(xi, yi, zi, cmap = plt.get_cmap('rainbow'))\n",
    "    if(iPlotLogscale):\n",
    "        plt.colorbar().set_label(label='log$_{10}$P',size=15)\n",
    "    else:\n",
    "        plt.colorbar().set_label(label='Probability',size=15)\n",
    "    # plot error bars\n",
    "    if(iPlotErrBars):\n",
    "        plt.errorbar(x=x_opt, y=y_opt, xerr=[[x_err_m], [x_err_p]], yerr=[[y_err_m], [y_err_p]], \n",
    "                     fmt='--o', ecolor='k', elinewidth=4, capsize=10, capthick=4)\n",
    "        plt.errorbar(x=x_opt, y=y_opt, xerr=[[x_err_m], [x_err_p]], yerr=[[y_err_m], [y_err_p]], \n",
    "                     fmt='--o', ecolor='w', elinewidth=2, capsize=8, capthick=2)\n",
    "    # plot optimal values\n",
    "    if(iPlotFit and iHaveOpt):\n",
    "        plt.scatter(x_opt, y_opt, marker = 'o', c = 'white', s = 100, zorder = 10)\n",
    "\n",
    "    # set x/y limits\n",
    "    plt.xlim(xmin, xmax)\n",
    "    plt.ylim(ymin, ymax)\n",
    "    # change x/y labels\n",
    "    plt.xlabel(x_name, fontsize=18)\n",
    "    plt.ylabel(y_name, fontsize=18)\n",
    "    # save figures to PDF and png\n",
    "    plt.savefig(\"bayesian_plot.pdf\", dpi=400)\n",
    "    plt.savefig(\"bayesian_plot.png\", dpi=400)\n",
    "    if(iPlotInteractive):\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
